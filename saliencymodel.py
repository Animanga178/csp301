# -*- coding: utf-8 -*-
"""SaliencyModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EL1GuC4Nd-nTQYyZ02CJKPXkTM-arjF5

# **'Where's Wally' with Machine Learning: Phase 4 - Saliency**

### Imports
"""

# Standard libraries
import os
from PIL import Image
import time
import json

# Torch & torchvision related
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision.transforms import transforms

# Matplotlib, numpy and visualisation
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2

from scipy.ndimage import zoom
from scipy.special import logsumexp
from scipy.ndimage import center_of_mass

from sklearn.metrics import roc_auc_score

!pip install pysaliency
import pysaliency
from pysaliency.plotting import visualize_distribution

# Third-party API
!pip install roboflow
from roboflow import Roboflow

import sys
sys.path.append('/content/DeepGaze')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DeepGazeIIE(pretrained=True).to(device)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print("Using:", device)

"""### Load in API dataset"""

# https://universe.roboflow.com/tan-rmi/whereiswaldo-ssmol/dataset/3
rf = Roboflow(api_key="iE53lBCUXtDffBO8i7dk")
project = rf.workspace("tan-rmi").project("whereiswaldo-ssmol")
version = project.version(3)
dataset = version.download("coco")
print(dataset.location)

"""### Transform data

### Custom preprocessing
"""

class PreprocessedImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(".jpg")]
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, image_path

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],  [0.229, 0.224, 0.225]),
])

"""### Load in and preprocess the test data"""

test_path = "/content/test"
test_dataset = PreprocessedImageDataset(test_path, transform)

test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

"""### Sample image post preprocessing"""

image, path = test_dataset[0]
print(f"Sample image path: {path}")
print(type(image))
plt.imshow(np.transpose(image.numpy(), (1, 2, 0)))
plt.axis('off')
plt.show()

"""### Download the centerbias template, resize it to match image size, apply `logsumexp` normalizationA"""

image.shape

"""### Load in the centerbias & output originals vs saliency overlay with bounding box"""

centerbias_template = np.load('/content/centerbias_mit1003.npy')
test_annotations_path = "/content/test/_annotations.coco.json"

with open(test_annotations_path, 'r') as f:
    coco_data = json.load(f)

annotations_by_id = {}
for ann in coco_data['annotations']:
    image_id = ann['image_id']
    annotations_by_id.setdefault(image_id, []).append(ann)

total_images = len(test_dataset)
for idx in range(total_images):
    image, path = test_dataset[idx]
    image_np = image.permute(1, 2, 0).cpu().numpy()
    image_np_uint8 = (image_np * 255).astype(np.uint8)

    _, H, W = image.shape
    zoom_factors = (H / centerbias_template.shape[0], W / centerbias_template.shape[1])
    centerbias = zoom(centerbias_template, zoom_factors, order=0, mode='nearest')
    centerbias -= logsumexp(centerbias)

    # Model prediction
    image_tensor = image.unsqueeze(0).to(device)
    centerbias_tensor = torch.tensor([centerbias]).to(device)
    with torch.no_grad():
        log_density_prediction = model(image_tensor, centerbias_tensor)
    saliency_map = torch.exp(log_density_prediction)[0, 0].cpu().numpy()

    # Convert saliency to heatmap
    saliency_resized = cv2.resize(saliency_map, (image_np.shape[1], image_np.shape[0]))
    heatmap = (saliency_resized * 255 / np.max(saliency_resized)).astype(np.uint8)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    overlay = cv2.addWeighted(image_np_uint8, 0.6, heatmap_color, 0.4, 0)

    # Find Wally's bbox from annotations
    image_filename = os.path.basename(path)
    image_id = next((img["id"] for img in coco_data["images"] if img["file_name"] == image_filename), None)
    wally_annotations = annotations_by_id.get(image_id, [])

    fig, axs = plt.subplots(1, 2, figsize=(12, 6))

    # Original
    axs[0].imshow(image_np_uint8)
    axs[0].set_title("Original Image")
    axs[0].axis("off")

    # Overlay with box
    axs[1].imshow(overlay)
    axs[1].set_title("Saliency Overlay + Wally")
    axs[1].axis("off")

    for ann in wally_annotations:
        x1, y1, w, h = ann["bbox"]
        x2 = x1 + w
        y2 = y1 + h
        rect = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='cyan', facecolor='none')
        axs[1].add_patch(rect)
        axs[1].text(x1, y1 - 10, "Wally", color='cyan', fontsize=12, backgroundcolor='black')

    plt.tight_layout()
    plt.show()

"""### Custom hit rate metric"""

wally_saliency_scores = []
hits = 0
total_images = len(test_dataset)

for idx in range(total_images):
    image, path = test_dataset[idx]

    # Find Wally's bbox from annotations
    image_filename = os.path.basename(path)
    image_id = next((img["id"] for img in coco_data["images"] if img["file_name"] == image_filename), None)
    wally_annotations = annotations_by_id.get(image_id, [])

    for ann in wally_annotations:
        x1, y1, w, h = map(int, ann["bbox"])
        x2 = x1 + w
        y2 = y1 + h

        # Calculate Wally saliency score
        saliency_crop = saliency_resized[y1:y2, x1:x2]
        wally_saliency_score = np.mean(saliency_crop)
        wally_saliency_scores.append(wally_saliency_score)

        # Calculate hit rate
        cy, cx = center_of_mass(saliency_resized)
        inside = x1 <= cx <= x2 and y1 <= cy <= y2
        if inside:
            hits += 1

wally_saliency_score_avg = np.mean(wally_saliency_scores)
hit_rate = hits / total_images
print(f"Average Wally Saliency Score: {wally_saliency_score_avg:.4f}")
print(f"Hit Rate: {hit_rate:.4f}")

"""### NSS metric output"""

nss_scores = []

for ann in wally_annotations:
    x, y, w, h = ann["bbox"]
    center_x = int(x + w / 2)
    center_y = int(y + h / 2)

    # Normalize saliency map
    mean_saliency = np.mean(saliency_resized)
    std_saliency = np.std(saliency_resized)

    nss = (saliency_resized[center_y, center_x] - mean_saliency) / std_saliency
    nss_scores.append(nss)

# Calculate average NSS
average_nss = np.mean(nss_scores)
print(f"Average NSS: {average_nss:.4f}")

"""### AUC metric output"""

# Create binary mask for Wally's bbox
mask = np.zeros(saliency_map.shape)
mask[y1:y2, x1:x2] = 1

# Flatten the saliency map and mask
saliency_flatten = saliency_map.flatten()
mask_flatten = mask.flatten()

# Calculate AUC
auc = roc_auc_score(mask_flatten, saliency_flatten)
print(f"AUC: {auc:.4f}")